{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7151bf81",
   "metadata": {},
   "source": [
    "# Data Collection "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b595bc",
   "metadata": {},
   "source": [
    "## 1. Firstly we collected ferilizer and yeild data from opendata pakistan  \n",
    "https://opendata.com.pk/dataset/fertilizer-usage-production-per-acre-across-punjab-2002-2015  \n",
    "`files/punjab_districts.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1c8ce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca464a9b",
   "metadata": {},
   "source": [
    "### Data exploration for data source 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd4be70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Province</th>\n",
       "      <th>Division</th>\n",
       "      <th>District</th>\n",
       "      <th>Usage (in 1000 nutirient tons)</th>\n",
       "      <th>Area Sown Total (Wheat)</th>\n",
       "      <th>Area Sown (Rice)</th>\n",
       "      <th>Area Sown (Cotton)</th>\n",
       "      <th>Area Sown (Sugarcane)</th>\n",
       "      <th>Wheat Production</th>\n",
       "      <th>Rice Production</th>\n",
       "      <th>Cotton Production</th>\n",
       "      <th>Sugarcane Production</th>\n",
       "      <th>Output/acre wheat</th>\n",
       "      <th>Output/Acre (Rice)</th>\n",
       "      <th>Output/Acre (Cotton)</th>\n",
       "      <th>Output/Acre (Sugarcane)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2002-03</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>Bahawalpur¬†Divn.</td>\n",
       "      <td>Bahawalpur</td>\n",
       "      <td>114</td>\n",
       "      <td>269</td>\n",
       "      <td>5.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>724</td>\n",
       "      <td>7</td>\n",
       "      <td>968.0</td>\n",
       "      <td>501</td>\n",
       "      <td>2.691450</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>3.611940</td>\n",
       "      <td>55.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2002-03</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>Bahawalpur¬†Divn.</td>\n",
       "      <td>Bahawalnagar</td>\n",
       "      <td>93</td>\n",
       "      <td>282</td>\n",
       "      <td>41.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>695</td>\n",
       "      <td>65</td>\n",
       "      <td>544.0</td>\n",
       "      <td>1155</td>\n",
       "      <td>2.464539</td>\n",
       "      <td>1.585366</td>\n",
       "      <td>3.056180</td>\n",
       "      <td>42.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002-03</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>Bahawalpur¬†Divn.</td>\n",
       "      <td>Rahim Yar Khan</td>\n",
       "      <td>168</td>\n",
       "      <td>303</td>\n",
       "      <td>13.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>740</td>\n",
       "      <td>19</td>\n",
       "      <td>1076.0</td>\n",
       "      <td>2620</td>\n",
       "      <td>2.442244</td>\n",
       "      <td>1.461538</td>\n",
       "      <td>3.504886</td>\n",
       "      <td>60.930233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002-03</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>D.G.Khan¬†Divn.</td>\n",
       "      <td>D.G. Khan</td>\n",
       "      <td>48</td>\n",
       "      <td>156</td>\n",
       "      <td>27.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>407</td>\n",
       "      <td>53</td>\n",
       "      <td>411.0</td>\n",
       "      <td>142</td>\n",
       "      <td>2.608974</td>\n",
       "      <td>1.962963</td>\n",
       "      <td>4.419355</td>\n",
       "      <td>47.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2002-03</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>D.G.Khan¬†Divn.</td>\n",
       "      <td>Layyah</td>\n",
       "      <td>48</td>\n",
       "      <td>178</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>379</td>\n",
       "      <td>1</td>\n",
       "      <td>63.0</td>\n",
       "      <td>674</td>\n",
       "      <td>2.129213</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.172414</td>\n",
       "      <td>42.125000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  Province                                Division         District  \\\n",
       "0  2002-03    Punjab  Bahawalpur¬†Divn.                            Bahawalpur   \n",
       "1  2002-03    Punjab  Bahawalpur¬†Divn.                          Bahawalnagar   \n",
       "2  2002-03    Punjab  Bahawalpur¬†Divn.                        Rahim Yar Khan   \n",
       "3  2002-03    Punjab                     D.G.Khan¬†Divn.            D.G. Khan   \n",
       "4  2002-03    Punjab                     D.G.Khan¬†Divn.               Layyah   \n",
       "\n",
       "   Usage (in 1000 nutirient tons)  Area Sown Total (Wheat)  Area Sown (Rice)  \\\n",
       "0                             114                      269               5.0   \n",
       "1                              93                      282              41.0   \n",
       "2                             168                      303              13.0   \n",
       "3                              48                      156              27.0   \n",
       "4                              48                      178               1.0   \n",
       "\n",
       "   Area Sown (Cotton)  Area Sown (Sugarcane)  Wheat Production   \\\n",
       "0               268.0                    9.0                724   \n",
       "1               178.0                   27.0                695   \n",
       "2               307.0                   43.0                740   \n",
       "3                93.0                    3.0                407   \n",
       "4                29.0                   16.0                379   \n",
       "\n",
       "   Rice Production   Cotton Production   Sugarcane Production  \\\n",
       "0                 7               968.0                   501   \n",
       "1                65               544.0                  1155   \n",
       "2                19              1076.0                  2620   \n",
       "3                53               411.0                   142   \n",
       "4                 1                63.0                   674   \n",
       "\n",
       "   Output/acre wheat  Output/Acre (Rice)  Output/Acre (Cotton)  \\\n",
       "0           2.691450            1.400000              3.611940   \n",
       "1           2.464539            1.585366              3.056180   \n",
       "2           2.442244            1.461538              3.504886   \n",
       "3           2.608974            1.962963              4.419355   \n",
       "4           2.129213            1.000000              2.172414   \n",
       "\n",
       "   Output/Acre (Sugarcane)  \n",
       "0                55.666667  \n",
       "1                42.777778  \n",
       "2                60.930233  \n",
       "3                47.333333  \n",
       "4                42.125000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.read_csv('punjab_districts.csv')\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06cd9ab",
   "metadata": {},
   "source": [
    "## 2. Secondly we get geojson data for boundaries of districts of punjab from kaggle  \n",
    "https://www.kaggle.com/datasets/idrisonkaggle/pakistan-districts-and-province-boundaries?resource=download  \n",
    "`files\\pakistan_districts_province_boundries.geojson`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61758144",
   "metadata": {},
   "source": [
    "### Cleaning the geojson data to get the required data only for the districts of punjab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b13b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded GeoJSON. Total rows: 148\n",
      "Columns found: ['objectid', 'province_territory', 'districts', 'shape_leng', 'shape_area', 'district_agency', 'status', 'cartodb_id', 'created_at', 'updated_at', 'geometry']\n",
      "‚ö†Ô∏è 'NAME_1' not found. Guessing province column is: 'province_territory'\n",
      "‚úÖ Filtered to Punjab. Districts found: 36\n",
      "‚ö†Ô∏è 'NAME_3' not found. Guessing district column is: 'districts'\n",
      "üéâ Success! Cleaned file saved as: punjab_districts_cleaned.geojson\n",
      "üëâ USE THIS FILE IN YOUR GOOGLE EARTH ENGINE CODE NOW.\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "input_file = 'files/pakistan_districts_province_boundries.geojson' \n",
    "\n",
    "try:\n",
    "    gdf = gpd.read_file(input_file)\n",
    "    print(f\"‚úÖ Loaded GeoJSON. Total rows: {len(gdf)}\")\n",
    "    print(f\"Columns found: {gdf.columns.tolist()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Filter for Punjab\n",
    "province_col = 'NAME_1'\n",
    "\n",
    "# Check if the column exists\n",
    "if province_col not in gdf.columns:\n",
    "    # Fallback search\n",
    "    possible_cols = [c for c in gdf.columns if 'prov' in c.lower() or 'name_1' in c.lower()]\n",
    "    if possible_cols:\n",
    "        province_col = possible_cols[0]\n",
    "    else:\n",
    "        print(\"‚ùå Could not find Province column. Please check the column list printed above.\")\n",
    "        exit()\n",
    "\n",
    "# Filter\n",
    "punjab_gdf = gdf[gdf[province_col].astype(str).str.contains('Punjab', case=False, na=False)].copy()\n",
    "print(f\"‚úÖ Filtered to Punjab. Districts found: {len(punjab_gdf)}\")\n",
    "\n",
    "# 3. Standardize District Names\n",
    "district_col = 'NAME_3'\n",
    "\n",
    "if district_col not in gdf.columns:\n",
    "    possible_cols = [c for c in gdf.columns if 'dist' in c.lower() or 'name_3' in c.lower()]\n",
    "    if possible_cols:\n",
    "        district_col = possible_cols[0]\n",
    "        \n",
    "\n",
    "# Function to match your CSV style (removing 'District' word, extra spaces)\n",
    "def clean_district_name(name):\n",
    "    if not name: return \"\"\n",
    "    name = str(name).strip()\n",
    "    name = re.sub(r'District', '', name, flags=re.IGNORECASE) \n",
    "    return name.strip()\n",
    "\n",
    "punjab_gdf['District_Name_Clean'] = punjab_gdf[district_col].apply(clean_district_name)\n",
    "\n",
    "# 4. Save the Cleaned File\n",
    "output_file = 'files/punjab_districts_cleaned.geojson'\n",
    "punjab_gdf.to_file(output_file, driver='GeoJSON')\n",
    "print(f\"üéâ Success! Cleaned file saved as: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f4841",
   "metadata": {},
   "source": [
    "## 3. Collecting weather and climate data from google earth engine "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85877789",
   "metadata": {},
   "source": [
    "Authenticate with google engine app on google cloud console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a237c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/drive%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=BcYFJmDaeWUQq_2yR-oQqliSRwbWn521EYas7ekz5Ns&tc=io86ae8Pz8h5IG0d_WcPbLqSwruVmDB3uSKVwSH62I4&cc=yetqqlZUeE_sPwpZt-3n5jfxG0C2eff-gDkUaQpIaVk>https://code.earthengine.google.com/client-auth?scopes=https%3A//www.googleapis.com/auth/earthengine%20https%3A//www.googleapis.com/auth/cloud-platform%20https%3A//www.googleapis.com/auth/drive%20https%3A//www.googleapis.com/auth/devstorage.full_control&request_id=BcYFJmDaeWUQq_2yR-oQqliSRwbWn521EYas7ekz5Ns&tc=io86ae8Pz8h5IG0d_WcPbLqSwruVmDB3uSKVwSH62I4&cc=yetqqlZUeE_sPwpZt-3n5jfxG0C2eff-gDkUaQpIaVk</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you should paste in the box below.</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import os\n",
    "\n",
    "# Delete old credentials if any\n",
    "cred_path = os.path.expanduser('~/.config/earthengine/credentials')\n",
    "if os.path.exists(cred_path):\n",
    "    os.remove(cred_path)\n",
    "\n",
    "# Force authentication with 'notebook' mode\n",
    "ee.Authenticate(auth_mode='notebook', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "name = os.environ.get('geeappname')\n",
    "ee.Initialize(project=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24714eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 36 districts.\n",
      "Sending request to Google Earth Engine... (This may take 1-2 mins)\n",
      "‚úÖ Success! NDVI data saved to 'punjab_ndvi_2002_2015.csv'\n",
      "  District  Mean_NDVI  Year\n",
      "0  Chiniot   0.451566  2002\n",
      "1  Chiniot   0.449537  2003\n",
      "2  Chiniot   0.465009  2004\n",
      "3  Chiniot   0.435443  2005\n",
      "4  Chiniot   0.477645  2006\n"
     ]
    }
   ],
   "source": [
    "import ee\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "# 2. Load Cleaned GeoJSON\n",
    "gdf = gpd.read_file('files/punjab_districts_cleaned.geojson')\n",
    "\n",
    "# Ensure it is in WGS84 (Lat/Lon) and keep only necessary columns\n",
    "if gdf.crs != 'EPSG:4326':\n",
    "    gdf = gdf.to_crs('EPSG:4326')\n",
    "\n",
    "# Keep only valid columns to avoid JSON errors\n",
    "gdf = gdf[['District_Name_Clean', 'geometry']]\n",
    "\n",
    "print(f\"Loaded {len(gdf)} districts.\")\n",
    "\n",
    "# 3. Convert to GEE FeatureCollection\n",
    "geojson_dict = json.loads(gdf.to_json())\n",
    "districts_fc = ee.FeatureCollection(geojson_dict['features'])\n",
    "\n",
    "# 4. Define NDVI Extraction Function\n",
    "def get_ndvi(feature):\n",
    "    district_name = feature.get('District_Name_Clean') \n",
    "    \n",
    "    # Range of years: 2002 to 2015\n",
    "    years = ee.List.sequence(2002, 2015) \n",
    "    \n",
    "    def get_yearly_ndvi(y):\n",
    "        y = ee.Number(y)\n",
    "        # Season: Oct 1st of Year Y to March 31st of Year Y+1\n",
    "        start = ee.Date.fromYMD(y, 10, 1)\n",
    "        end = ee.Date.fromYMD(y.add(1), 3, 31)\n",
    "        \n",
    "        # MOD13A2: 16-Day NDVI\n",
    "        ndvi = ee.ImageCollection('MODIS/006/MOD13A2') \\\n",
    "                 .filterDate(start, end) \\\n",
    "                 .select('NDVI') \\\n",
    "                 .mean() \\\n",
    "                 .clip(feature.geometry())\n",
    "        \n",
    "        # Reduce to a single number (Mean NDVI for the district)\n",
    "        # We use 'bestEffort=True' to avoid \"Image too large\" errors\n",
    "        mean_val = ndvi.reduceRegion(\n",
    "            reducer=ee.Reducer.mean(),\n",
    "            geometry=feature.geometry(),\n",
    "            scale=1000,\n",
    "            bestEffort=True\n",
    "        ).get('NDVI')\n",
    "        \n",
    "        return ee.Feature(None, {\n",
    "            'District': district_name,\n",
    "            'Year': y,\n",
    "            'Mean_NDVI': mean_val\n",
    "        })\n",
    "    \n",
    "  \n",
    "    return ee.FeatureCollection(years.map(get_yearly_ndvi))\n",
    "\n",
    "# 5. Run Extraction (Server Side)\n",
    "print(\"Sending request to Google Earth Engine... (This may take 1-2 mins)\")\n",
    "\n",
    "results = districts_fc.map(get_ndvi).flatten()\n",
    "\n",
    "try:\n",
    "    data = results.getInfo()['features']\n",
    "    ndvi_list = [d['properties'] for d in data]\n",
    "\n",
    "    df_ndvi = pd.DataFrame(ndvi_list)\n",
    "\n",
    "    # Scale NDVI (MODIS is 0-10000, we want 0-1)\n",
    "    if not df_ndvi.empty:\n",
    "        df_ndvi['Mean_NDVI'] = df_ndvi['Mean_NDVI'] / 10000.0\n",
    "        df_ndvi.to_csv('files/punjab_ndvi_2002_2015.csv', index=False)\n",
    "        print(\" Success! NDVI data saved to 'files/punjab_ndvi_2002_2015.csv'\")\n",
    "        print(df_ndvi.head())\n",
    "    else:\n",
    "        print(\" Error: Resulting DataFrame is empty. Check your District Names.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Error extracting data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144df2c5",
   "metadata": {},
   "source": [
    "### Data integrations: Calculating geometric centroids for each district from the GeoJSON file and automating the retrieval of daily historical weather data via the NASA POWER API, which is then aggregated into yearly total rainfall and average temperature features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1deede7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 36 districts for weather fetching.\n",
      "üöÄ Starting NASA POWER downloads for 36 districts...\n",
      "‚è≥ This might take 2-3 minutes. Please wait...\n",
      "[1/36] Fetching climate for: Chiniot...\n",
      "[2/36] Fetching climate for: Lahore...\n",
      "[3/36] Fetching climate for: Bhakkar...\n",
      "[4/36] Fetching climate for: Gujrat...\n",
      "[5/36] Fetching climate for: Mandi Bahauddin...\n",
      "[6/36] Fetching climate for: Hafizabad...\n",
      "[7/36] Fetching climate for: Khushab...\n",
      "[8/36] Fetching climate for: Attock...\n",
      "[9/36] Fetching climate for: Jhang...\n",
      "[10/36] Fetching climate for: Bahawalnagar...\n",
      "[11/36] Fetching climate for: Bahawalpur...\n",
      "[12/36] Fetching climate for: Chakwal...\n",
      "[13/36] Fetching climate for: Dera Ghazi Khan...\n",
      "[14/36] Fetching climate for: Faisalabad...\n",
      "[15/36] Fetching climate for: Gujranwala...\n",
      "[16/36] Fetching climate for: Jhelum...\n",
      "[17/36] Fetching climate for: Kasur...\n",
      "[18/36] Fetching climate for: Khanewal...\n",
      "[19/36] Fetching climate for: Layyah...\n",
      "[20/36] Fetching climate for: Lodhran...\n",
      "[21/36] Fetching climate for: Mianwali...\n",
      "[22/36] Fetching climate for: Multan...\n",
      "[23/36] Fetching climate for: Muzaffargarh...\n",
      "[24/36] Fetching climate for: Nankana Sahib...\n",
      "[25/36] Fetching climate for: Narowal...\n",
      "[26/36] Fetching climate for: Okara...\n",
      "[27/36] Fetching climate for: Pakpattan...\n",
      "[28/36] Fetching climate for: Rahim Yar Khan...\n",
      "[29/36] Fetching climate for: Rajanpur...\n",
      "[30/36] Fetching climate for: Rawalpindi...\n",
      "[31/36] Fetching climate for: Sahiwal...\n",
      "[32/36] Fetching climate for: Sargodha...\n",
      "[33/36] Fetching climate for: Sheikhupura...\n",
      "[34/36] Fetching climate for: Sialkot...\n",
      "[35/36] Fetching climate for: Toba Tek Singh...\n",
      "[36/36] Fetching climate for: Vehari...\n",
      "\n",
      "üéâ Success! Climate data saved to 'punjab_climate_2002_2015.csv'\n",
      "  District  Year  Total_Rainfall_mm  Avg_Temp_C\n",
      "0  Chiniot  2002             191.33       27.67\n",
      "1  Chiniot  2003             326.81       26.41\n",
      "2  Chiniot  2004             307.88       27.46\n",
      "3  Chiniot  2005             419.59       25.76\n",
      "4  Chiniot  2006             450.55       26.37\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# 1. Load GeoJSON to get Coordinates\n",
    "try:\n",
    "    gdf = gpd.read_file('punjab_districts_cleaned.geojson')\n",
    "    \n",
    "    # Ensure it is in WGS84 (Lat/Lon)\n",
    "    if gdf.crs != 'EPSG:4326':\n",
    "        gdf = gdf.to_crs('EPSG:4326')\n",
    "\n",
    "\n",
    "    gdf_projected = gdf.to_crs('EPSG:3857') \n",
    "    gdf['centroid'] = gdf_projected.geometry.centroid.to_crs('EPSG:4326')\n",
    "    \n",
    "    # Create a list of districts with their Lat/Lon\n",
    "    districts_info = []\n",
    "    for idx, row in gdf.iterrows():\n",
    "        # Try to find the district name column\n",
    "        d_name = row.get('District_Name_Clean', row.get('District', row.get('NAME_3', f\"District_{idx}\")))\n",
    "        \n",
    "        districts_info.append({\n",
    "            'District': d_name,\n",
    "            'Lat': round(row.centroid.y, 4),\n",
    "            'Lon': round(row.centroid.x, 4)\n",
    "        })\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(districts_info)} districts for weather fetching.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Error loading GeoJSON: {e}\")\n",
    "    sys.exit()\n",
    "\n",
    "# 2. Define NASA POWER API Function\n",
    "def fetch_weather(lat, lon, start_year=2002, end_year=2015):\n",
    "    base_url = \"https://power.larc.nasa.gov/api/temporal/daily/point\"\n",
    "    \n",
    "    params = {\n",
    "\n",
    "        'parameters': 'PRECTOTCORR,T2M', \n",
    "        'community': 'AG',\n",
    "        'longitude': lon,\n",
    "        'latitude': lat,\n",
    "        'start': f\"{start_year}0101\",\n",
    "        'end': f\"{end_year}1231\",\n",
    "        'format': 'JSON'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è API Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# 3. Loop and Download\n",
    "weather_data = []\n",
    "\n",
    "print(f\" Starting NASA POWER downloads for {len(districts_info)} districts...\")\n",
    "\n",
    "\n",
    "for i, dist in enumerate(districts_info):\n",
    "    name = dist['District']\n",
    "    print(f\"[{i+1}/{len(districts_info)}] Fetching climate for: {name}...\")\n",
    "    \n",
    "    data = fetch_weather(dist['Lat'], dist['Lon'])\n",
    "    \n",
    "    if data:\n",
    "        try:\n",
    "            # Parse the JSON response\n",
    "            properties = data['properties']['parameter']\n",
    "            \n",
    "            # Note: NASA uses the exact parameter name in the response\n",
    "            rain = properties['PRECTOTCORR'] \n",
    "            temp = properties['T2M']\n",
    "            \n",
    "            # Aggregate Daily Data to Yearly\n",
    "            for year in range(2002, 2016):\n",
    "                year_str = str(year)\n",
    "                \n",
    "                # Filter values for the specific year\n",
    "                yearly_rain_vals = [val for date, val in rain.items() if date.startswith(year_str) and val != -999.0]\n",
    "                yearly_temp_vals = [val for date, val in temp.items() if date.startswith(year_str) and val != -999.0]\n",
    "                \n",
    "                if yearly_rain_vals:\n",
    "                    total_rain = sum(yearly_rain_vals)\n",
    "                    avg_temp = sum(yearly_temp_vals) / len(yearly_temp_vals)\n",
    "                    \n",
    "                    weather_data.append({\n",
    "                        'District': name,\n",
    "                        'Year': year,\n",
    "                        'Total_Rainfall_mm': round(total_rain, 2),\n",
    "                        'Avg_Temp_C': round(avg_temp, 2)\n",
    "                    })\n",
    "        except KeyError as e:\n",
    "            print(f\" Error parsing data for {name}: {e}\")\n",
    "        \n",
    "    time.sleep(1.0)\n",
    "\n",
    "# 4. Save to CSV\n",
    "if weather_data:\n",
    "    df_weather = pd.DataFrame(weather_data)\n",
    "    output_filename = 'files/punjab_climate_2002_2015.csv'\n",
    "    df_weather.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nüéâ Success! Climate data saved to '{output_filename}'\")\n",
    "    print(df_weather.head())\n",
    "else:\n",
    "    print(\"\\n Failed to collect weather data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1394bb4",
   "metadata": {},
   "source": [
    "## Data cleaning : Standardizing col names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25379c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded original file: punjab_districts.csv\n",
      "‚úÖ Success! Generated: cleaned_punjab_agri_2002_2015_V2.csv\n",
      "   Rows: 416\n",
      "üëâ Now run the Health Check script again.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "raw_file_name = 'files/punjab_districts.csv' \n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(raw_file_name)\n",
    "    print(f\"Loaded original file: {raw_file_name}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå ERROR: Could not find '{raw_file_name}'. Please rename the variable above.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "df.columns = [\n",
    "    'Year_Range', 'Province', 'Division', 'District', 'Fertilizer_Usage_K_Tons',\n",
    "    'Area_Sown_Wheat', 'Area_Sown_Rice', 'Area_Sown_Cotton', 'Area_Sown_Sugarcane',\n",
    "    'Production_Wheat_Tons', 'Production_Rice_Tons', 'Production_Cotton_Tons', \n",
    "    'Production_Sugarcane_Tons', 'Yield_Wheat_Acre', 'Yield_Rice_Acre', \n",
    "    'Yield_Cotton_Acre', 'Yield_Sugarcane_Acre'\n",
    "]\n",
    "\n",
    "\n",
    "df['Year'] = df['Year_Range'].astype(str).str.split('-').str[0].astype(int)\n",
    "\n",
    "# Fix District Names (Remove 'Divn.', extra spaces)\n",
    "def clean_name(name):\n",
    "    if pd.isna(name): return name\n",
    "    name = str(name).strip()\n",
    "    name = re.sub(r'\\s*Divn\\.', '', name, flags=re.IGNORECASE) # Remove \"Divn.\"\n",
    "    name = re.sub(r'[^a-zA-Z\\s]', '', name) # Remove special chars\n",
    "    return name.strip()\n",
    "\n",
    "df['District'] = df['District'].apply(clean_name)\n",
    "\n",
    "# --- 4. Save the V2 File ---\n",
    "output_name = 'files/cleaned_punjab_agri_2002_2015_V2.csv'\n",
    "df.to_csv(output_name, index=False)\n",
    "\n",
    "print(f\" Success! Generated: {output_name}\")\n",
    "print(f\"   Rows: {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a43886b",
   "metadata": {},
   "source": [
    "## Data integrity verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b9c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä DATA HEALTH CHECK REPORT\n",
      "==============================\n",
      "\n",
      "checking: Agri Data (cleaned_punjab_agri_2002_2015_V2.csv)...\n",
      "   ‚úÖ Found! Rows: 416, Columns: 18\n",
      "   ‚úÖ Merge Keys (District, Year) present.\n",
      "   üëÄ Columns: ['Year_Range', 'Province', 'Division', 'District', 'Fertilizer_Usage_K_Tons', 'Area_Sown_Wheat', 'Area_Sown_Rice', 'Area_Sown_Cotton', 'Area_Sown_Sugarcane', 'Production_Wheat_Tons', 'Production_Rice_Tons', 'Production_Cotton_Tons', 'Production_Sugarcane_Tons', 'Yield_Wheat_Acre', 'Yield_Rice_Acre', 'Yield_Cotton_Acre', 'Yield_Sugarcane_Acre', 'Year']\n",
      "\n",
      "checking: NDVI Data (punjab_ndvi_2002_2015.csv)...\n",
      "   ‚úÖ Found! Rows: 504, Columns: 3\n",
      "   ‚úÖ Merge Keys (District, Year) present.\n",
      "   üëÄ Columns: ['District', 'Mean_NDVI', 'Year']\n",
      "\n",
      "checking: Climate Data (punjab_climate_2002_2015.csv)...\n",
      "   ‚úÖ Found! Rows: 504, Columns: 4\n",
      "   ‚úÖ Merge Keys (District, Year) present.\n",
      "   üëÄ Columns: ['District', 'Year', 'Total_Rainfall_mm', 'Avg_Temp_C']\n",
      "\n",
      "==============================\n",
      "üéâ ALL SYSTEMS GO! You have all 3 datasets. We can merge now.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define your file names\n",
    "files = {\n",
    "    \"Agri\": \"files/cleaned_punjab_agri_2002_2015_V2.csv\",\n",
    "    \"NDVI\": \"files/punjab_ndvi_2002_2015.csv\",\n",
    "    \"Climate\": \"files/punjab_climate_2002_2015.csv\"\n",
    "}\n",
    "\n",
    "print(\" DATA HEALTH CHECK REPORT\\n\" + \"=\"*30)\n",
    "\n",
    "success_count = 0\n",
    "\n",
    "for name, filename in files.items():\n",
    "    print(f\"\\nchecking: {name} Data ({filename})...\")\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            df = pd.read_csv(filename)\n",
    "            \n",
    "            # 1. Check Row Count\n",
    "            print(f\"    Found! Rows: {len(df)}, Columns: {len(df.columns)}\")\n",
    "            \n",
    "            # 2. Check Key Columns for Merging\n",
    "            cols = [c.lower() for c in df.columns]\n",
    "            has_district = any('dist' in c for c in cols)\n",
    "            has_year = any('year' in c for c in cols)\n",
    "            \n",
    "            if has_district and has_year:\n",
    "                print(\"    Merge Keys (District, Year) present.\")\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(f\"    CRITICAL WARNING: Missing 'District' or 'Year' column! Found: {df.columns.tolist()}\")\n",
    "            \n",
    "            # 3. Show a sneak peek\n",
    "            print(f\"    Columns: {df.columns.tolist()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error reading file: {e}\")\n",
    "    else:\n",
    "        print(f\"    FILE MISSING: The file '{filename}' was not found in this folder.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "if success_count == 3:\n",
    "    print(\"  all 3 datasets VERIFIED. We can merge now.\")\n",
    "else:\n",
    "    print(f\" WAIT! Only {success_count}/3 datasets are ready. Fix the missing/broken ones first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a45bc74",
   "metadata": {},
   "source": [
    "# Data merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd38ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All 3 files loaded successfully.\n",
      "\n",
      "========================================\n",
      "üéâ MASTER DATASET CREATED: final_dataset_ml_ready.csv\n",
      "   Total Rows: 392\n",
      "   Features: ['Year_Range', 'Province', 'Division', 'District', 'Fertilizer_Usage_K_Tons', 'Area_Sown_Wheat', 'Area_Sown_Rice', 'Area_Sown_Cotton', 'Area_Sown_Sugarcane', 'Production_Wheat_Tons', 'Production_Rice_Tons', 'Production_Cotton_Tons', 'Production_Sugarcane_Tons', 'Yield_Wheat_Acre', 'Yield_Rice_Acre', 'Yield_Cotton_Acre', 'Yield_Sugarcane_Acre', 'Year', 'Mean_NDVI', 'District', 'Total_Rainfall_mm', 'Avg_Temp_C']\n",
      "========================================\n",
      "üëâ You are now ready for the ML Pipeline!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load the 3 Datasets\n",
    "try:\n",
    "    df_agri = pd.read_csv(\"cleaned_punjab_agri_2002_2015_V2.csv\")\n",
    "    df_ndvi = pd.read_csv(\"punjab_ndvi_2002_2015.csv\")\n",
    "    df_climate = pd.read_csv(\"punjab_climate_2002_2015.csv\")\n",
    "    print(\"‚úÖ All 3 files loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"‚ùå Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Standardize Keys (District Name Cleaning)\n",
    "def standardize_name(name):\n",
    "    name = str(name).lower().strip()\n",
    "    name = re.sub(r'district', '', name) # Remove 'district' word\n",
    "    name = re.sub(r'[^a-z]', '', name)   # Keep only letters\n",
    "    return name\n",
    "\n",
    "# Apply to all 3 dataframes\n",
    "df_agri['Merge_Key'] = df_agri['District'].apply(standardize_name)\n",
    "df_ndvi['Merge_Key'] = df_ndvi['District'].apply(standardize_name)\n",
    "df_climate['Merge_Key'] = df_climate['District'].apply(standardize_name)\n",
    "\n",
    "# 3. Perform the Merge\n",
    "# Merge Agri + NDVI\n",
    "df_merged = pd.merge(df_agri, df_ndvi, on=['Merge_Key', 'Year'], how='inner')\n",
    "\n",
    "# Merge Result + Climate\n",
    "df_final = pd.merge(df_merged, df_climate, on=['Merge_Key', 'Year'], how='inner')\n",
    "\n",
    "# 4. Clean Up\n",
    "# Drop extra columns (like duplicated District names from merge)\n",
    "# We keep the original 'District_x' as the main District name\n",
    "df_final = df_final.rename(columns={'District_x': 'District'})\n",
    "cols_to_drop = [c for c in df_final.columns if 'District_y' in c or 'Merge_Key' in c]\n",
    "df_final = df_final.drop(columns=cols_to_drop)\n",
    "\n",
    "# 5. Save Master Dataset\n",
    "output_file = \"final_dataset_ml_ready.csv\"\n",
    "df_final.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(f\" MASTER DATASET CREATED: {output_file}\")\n",
    "print(f\"   Total Rows: {len(df_final)}\")\n",
    "print(f\"   Features: {df_final.columns.tolist()}\")\n",
    "print(\"=\"*40)\n",
    "print(\" Data ready for the ML Pipeline!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46c5f64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
